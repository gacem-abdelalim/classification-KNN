{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Abdel\n",
      "[nltk_data]     Alim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) #stop words are wor\n",
    "def remove_stop(x):\n",
    "    return \" \".join([word for word in str(x).split() if word not in stop_words])\n",
    "def remove_multiple_spaces(x):\n",
    "    return \" \".join([word for word in str(x).split()])\n",
    "def preprocessing(data):\n",
    "    data = data.drop(['COMMENT_ID', 'AUTHOR', 'DATE'], axis=1)\n",
    "    data['CONTENT'] = data['CONTENT'].str.lower()\n",
    "    data['CONTENT'] = data['CONTENT'].str.replace(r'.com/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),])+',' ')\n",
    "    data['CONTENT'] = data['CONTENT'].str.replace(r'[^\\w\\s]',' ')\n",
    "    data['CONTENT'] = data['CONTENT'].apply(lambda x : remove_multiple_spaces(x))\n",
    "    data['CONTENT'] = data['CONTENT'].apply(lambda x : remove_stop(x))\n",
    "    data = data.drop_duplicates(subset='CONTENT', keep='first')\n",
    "    x = data['CONTENT']\n",
    "    y = data['CLASS']\n",
    "    return(x,y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameter tuning with one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hidden_layer(X_main,y_main,iter,noeuds_hidden_layers, alphas):\n",
    "    meanAll = []\n",
    "    for lay in noeuds_hidden_layers:\n",
    "        accuracyvalid= []\n",
    "        for alpha in alphas:\n",
    "            mlp = MLPClassifier(solver=\"lbfgs\", random_state=1,hidden_layer_sizes=[lay], alpha=alpha)\n",
    "            for i in range(iter):\n",
    "                X_train, X_validat, y_train, y_validat = train_test_split(X_main,y_main ,\n",
    "                                test_size = 0.16,\n",
    "                                shuffle = True)\n",
    "                mlp.fit(X_train, y_train)\n",
    "                accuracy_valid = mlp.score(X_validat,y_validat)\n",
    "                accuracyvalid.append(accuracy_valid)\n",
    "            \n",
    "            f = np.array(accuracyvalid)\n",
    "        \n",
    "            a = [lay,alpha,f.mean()]\n",
    "            meanAll.append(a)\n",
    "    pm = np.array(meanAll)\n",
    "    best_number_of_noeds = np.where(pm[:,-1] ==max(pm[:,-1]))[0]\n",
    "    return meanAll[best_number_of_noeds[0]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameter tuning with two hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def two_hidden_layer(X_main,y_main,iter,noeuds_hidden_layers, alphas):\n",
    "    meanAll = []\n",
    "    for lay1 in noeuds_hidden_layers:\n",
    "        for lay2 in noeuds_hidden_layers:\n",
    "            accuracyvalid= []\n",
    "            for alpha in alphas:\n",
    "                mlp = MLPClassifier(solver=\"lbfgs\", random_state=1,hidden_layer_sizes=[lay1,lay2], alpha=alpha)\n",
    "                for i in range(iter):\n",
    "                    X_train, X_validat, y_train, y_validat = train_test_split(X_main,y_main ,\n",
    "                                    test_size = 0.16,\n",
    "                                    shuffle = True)\n",
    "                    mlp.fit(X_train, y_train)\n",
    "                    accuracy_valid = mlp.score(X_validat,y_validat)\n",
    "                    accuracyvalid.append(accuracy_valid)\n",
    "                \n",
    "                f = np.array(accuracyvalid)\n",
    "            \n",
    "                a = [lay1,lay2,alpha,f.mean()]\n",
    "                meanAll.append(a)\n",
    "    pm = np.array(meanAll)\n",
    "    best_number_of_noeds = np.where(pm[:,-1] ==max(pm[:,-1]))[0]\n",
    "    return meanAll[best_number_of_noeds[0]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameter tuning with three hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_hidden_layer(X_main,y_main,iter,noeuds_hidden_layers, alphas):\n",
    "    meanAll = []\n",
    "    for lay1 in noeuds_hidden_layers:\n",
    "        for lay2 in noeuds_hidden_layers:\n",
    "            for lay3 in noeuds_hidden_layers:\n",
    "                accuracyvalid= []\n",
    "                for alpha in alphas:\n",
    "                    mlp = MLPClassifier(solver=\"lbfgs\", random_state=1,hidden_layer_sizes=[lay1,lay2,lay3], alpha=alpha)\n",
    "                    for i in range(iter):\n",
    "                        X_train, X_validat, y_train, y_validat = train_test_split(X_main,y_main ,\n",
    "                                        test_size = 0.16,\n",
    "                                        shuffle = True)\n",
    "                        mlp.fit(X_train, y_train)\n",
    "                        accuracy_valid = mlp.score(X_validat,y_validat)\n",
    "                        accuracyvalid.append(accuracy_valid)\n",
    "                    \n",
    "                    f = np.array(accuracyvalid)\n",
    "                \n",
    "                    a = [lay1,lay2,lay3,alpha,f.mean()]\n",
    "                    meanAll.append(a)\n",
    "    pm = np.array(meanAll)\n",
    "    best_number_of_noeds = np.where(pm[:,-1] ==max(pm[:,-1]))[0]\n",
    "    return meanAll[best_number_of_noeds[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_hidden_layer_we_choose(X_main,y_main,best_one_hidden_layer,best_two_hidden_layer,best_three_hidden_layer):\n",
    "    X_train, X_validat, y_train, y_validat = train_test_split(X_main,y_main ,\n",
    "                            test_size = 0.16,\n",
    "                            shuffle = True)\n",
    "    \n",
    "    a = best_one_hidden_layer\n",
    "    b = best_two_hidden_layer\n",
    "    c = best_three_hidden_layer\n",
    "\n",
    "    modle = MLPClassifier(solver=\"lbfgs\", random_state=1,hidden_layer_sizes=[a[0]], alpha=a[1])\n",
    "    modle.fit(X_train, y_train)\n",
    "    a[-1] = modle.score(X_validat,y_validat)\n",
    "    modle = MLPClassifier(solver=\"lbfgs\", random_state=1,hidden_layer_sizes=[b[0],b[1]], alpha=b[2])\n",
    "    modle.fit(X_train, y_train)\n",
    "    b[-1] = modle.score(X_validat,y_validat)\n",
    "    modle = MLPClassifier(solver=\"lbfgs\", random_state=1,hidden_layer_sizes=[c[0],c[1],c[2]], alpha=c[3])\n",
    "    modle.fit(X_train, y_train)\n",
    "    c[-1] = modle.score(X_validat,y_validat)\n",
    "\n",
    "    list2 = [a[-1], b[-1], c[-1]]\n",
    "    max1 = max(list2)\n",
    "    index = list2.index(max1)\n",
    "    if index == 0:\n",
    "        return a\n",
    "    if index == 1:\n",
    "        return b\n",
    "    if index == 2:\n",
    "        return c\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 0.001, 0.9636363636363636]\n",
      "[15, 10, 0.001, 0.9818181818181818]\n",
      "[10, 3, 10, 0.001, 0.9818181818181818]\n",
      "[15, 10, 0.001]\n",
      "accuracy test 0.9117647058823529\n",
      "[[32  5]\n",
      " [ 1 30]]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Youtube01-Psy.csv')\n",
    "x, y = preprocessing(data)\n",
    "\n",
    "vectorizer = CountVectorizer( )\n",
    "featuers = vectorizer.fit_transform(x)\n",
    "\n",
    "\n",
    "X_main, X_test, y_main, y_test = train_test_split(featuers,y ,\n",
    "                                   test_size = 0.2, \n",
    "                                   shuffle = True)\n",
    "\n",
    "noeuds_hidden_layers = [1,3,5,10,15,20]\n",
    "alphas = [0.001,0.01,0.5]\n",
    "iter = 100\n",
    "\n",
    "\n",
    "X_train, X_validat, y_train, y_validat = train_test_split(X_main,y_main ,\n",
    "                            test_size = 0.16,\n",
    "                            shuffle = True)\n",
    "\n",
    "a = one_hidden_layer(X_main,y_main,iter,noeuds_hidden_layers, alphas)\n",
    "b = two_hidden_layer(X_main,y_main,iter,noeuds_hidden_layers, alphas)\n",
    "c = three_hidden_layer(X_main,y_main,iter,noeuds_hidden_layers, alphas)\n",
    "\n",
    "best = which_hidden_layer_we_choose(X_main, y_main, a, b, c)\n",
    "print(best[:-1])\n",
    "modle = MLPClassifier(solver=\"lbfgs\", random_state=1,hidden_layer_sizes=best[:-2], alpha=best[-2])\n",
    "modle.fit(X_train, y_train)\n",
    "print(\"accuracy test\", modle.score(X_test,y_test))\n",
    "pred_test = modle.predict(X_test)\n",
    "print(confusion_matrix(y_test,pred_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'myModel.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Abdel Alim\\Documents\\GitHub\\classification-KNN\\final folder\\finalNN.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Abdel%20Alim/Documents/GitHub/classification-KNN/final%20folder/finalNN.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#save the model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Abdel%20Alim/Documents/GitHub/classification-KNN/final%20folder/finalNN.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m modle \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mmyModel.joblib\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Abdel Alim\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py:579\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    577\u001b[0m         obj \u001b[39m=\u001b[39m _unpickle(fobj)\n\u001b[0;32m    578\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 579\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(filename, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    580\u001b[0m         \u001b[39mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[39mas\u001b[39;00m fobj:\n\u001b[0;32m    581\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fobj, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    582\u001b[0m                 \u001b[39m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    583\u001b[0m                 \u001b[39m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    584\u001b[0m                 \u001b[39m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'myModel.joblib'"
     ]
    }
   ],
   "source": [
    "#save the model\n",
    "modle = joblib.load('myModel.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66abd9c73c372f28a8ef98b8462078e3c895e5bf5441957c690bca4abf6b5117"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
